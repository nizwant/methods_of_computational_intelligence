{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    __slots__ = [\n",
    "        # weights \n",
    "        \"weights\",\n",
    "        \"weights_gradient\",\n",
    "        \"weights_momentum\",\n",
    "        \"weights_gradient_squared\",\n",
    "        \"weights_changes\",\n",
    "\n",
    "        # biases\n",
    "        \"biases\",\n",
    "        \"biases_gradient\",\n",
    "        \"biases_changes\",\n",
    "        \"biases_momentum\",\n",
    "        \"biases_gradient_squared\",\n",
    "\n",
    "        # activation functions\n",
    "        \"activation\",\n",
    "        \"activation_derivative\",\n",
    "\n",
    "        # backpropagation\n",
    "        \"delta\",\n",
    "        \"a\",\n",
    "        \"z\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, nodes_in, nodes_out, activation=\"sigmoid\"):\n",
    "        self.weights = np.random.normal(size=(nodes_in, nodes_out), scale=1)\n",
    "        self.biases = np.random.normal(size=(1, nodes_out))\n",
    "        self.biases_gradient = np.zeros(self.biases.shape)\n",
    "        self.weights_gradient = np.zeros(self.weights.shape)\n",
    "        self.biases_changes = np.zeros(self.biases.shape)\n",
    "        self.weights_changes = np.zeros(self.weights.shape)\n",
    "        self.biases_momentum = np.zeros(self.biases.shape)\n",
    "        self.weights_momentum = np.zeros(self.weights.shape)\n",
    "        self.biases_gradient_squared = np.zeros(self.biases.shape)\n",
    "        self.weights_gradient_squared = np.zeros(self.weights.shape)\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = self.sigmoid\n",
    "            self.activation_derivative = self.sigmoid_derivative\n",
    "        elif activation == \"linear\":\n",
    "            self.activation = self.linear\n",
    "            self.activation_derivative = self.linear_derivative\n",
    "\n",
    "    def calculate_layer(self, input):\n",
    "        \"\"\"\n",
    "        Calculate the output of the layer\n",
    "        Takes in a numpy array and returns a numpy array\n",
    "        \"\"\"\n",
    "        return self.activation(np.dot(input, self.weights) + self.biases)\n",
    "\n",
    "    def calculate_layer_and_before_activation(self, input):\n",
    "        \"\"\"\n",
    "        Calculate the output of the layer and the output before the activation function\n",
    "        Takes in a numpy array and returns a numpy array\n",
    "        \"\"\"\n",
    "        return np.dot(input, self.weights) + self.biases, self.activation(np.dot(input, self.weights) + self.biases)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function\n",
    "        Takes in a numpy array and returns a numpy array\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid derivative function\n",
    "        Takes in a numpy array and returns a numpy array\n",
    "        \"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def linear(self, x):\n",
    "        \"\"\"\n",
    "        Linear activation function\n",
    "        Takes in a numpy array and returns a numpy array\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "    def linear_derivative(self, x):\n",
    "        \"\"\"\n",
    "        Linear derivative function\n",
    "        Takes in a numpy array and returns a numpy array\n",
    "        \"\"\"\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    def report_layer(self, layer_num):\n",
    "        return (\n",
    "            f\"Layer number {layer_num}\\nWeights\\n{self.weights}\\nbiases\\n{self.biases}\"\n",
    "        )\n",
    "\n",
    "    def apply_gradient(self, learning_rate):\n",
    "        self.weights -= learning_rate * self.weights_gradient\n",
    "        self.biases -= learning_rate * self.biases_gradient\n",
    "\n",
    "    def apply_changes(self):\n",
    "        self.weights += self.weights_changes\n",
    "        self.biases += self.biases_changes\n",
    "\n",
    "    def calculate_momentum(self, momentum_decay):\n",
    "        self.biases_momentum =  momentum_decay * self.biases_momentum + (1 - momentum_decay) * self.biases_gradient\n",
    "        self.weights_momentum = momentum_decay * self.weights_momentum + (1 - momentum_decay) * self.weights_gradient\n",
    "\n",
    "    def calculate_gradient_squared(self, decay_rate):\n",
    "        self.biases_gradient_squared = decay_rate * self.biases_gradient_squared + (1 - decay_rate) * self.biases_gradient ** 2\n",
    "        self.weights_gradient_squared = decay_rate * self.weights_gradient_squared + (1 - decay_rate) * self.weights_gradient ** 2\n",
    "\n",
    "    def calculate_changes(self, learning_rate, epsilon, t, momentum_decay, decay_rate):\n",
    "        self.weights_changes = (- learning_rate / (np.sqrt(self.weights_gradient_squared / (\n",
    "            1 - decay_rate **t\n",
    "        )) + epsilon)) * self.weights_momentum / (1 - momentum_decay**t)\n",
    "\n",
    "        self.biases_changes = (\n",
    "            - learning_rate / (np.sqrt(self.biases_gradient_squared / (\n",
    "            1 - decay_rate **t) + epsilon)\n",
    "        ) * self.biases_momentum / (1 - momentum_decay**t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    __slots__ = [\"hidden_layers\", \"layers\"]\n",
    "\n",
    "    def __init__(self, hidden_layers, input_size, output_size):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.layers = []\n",
    "\n",
    "        # Create the input layer\n",
    "        input_layer = Layer(1, hidden_layers[0])\n",
    "        self.layers.append(input_layer)\n",
    "\n",
    "        # Create the hidden layers\n",
    "        for input_size, output_size in zip(hidden_layers, hidden_layers[1:]):\n",
    "            self.layers.append(Layer(input_size, output_size))\n",
    "\n",
    "        # Create the output layer\n",
    "        output_layer = Layer(hidden_layers[-1], 1, activation=\"linear\")\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes a input and returns the output of the network\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.calculate_layer(input)\n",
    "        return input\n",
    "\n",
    "    def train(\n",
    "        self, input, output, learning_rate=0.003, batch_size_frac=0.1, epochs=30000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train the network on a given dataset\n",
    "        \"\"\"\n",
    "        mse_list = []\n",
    "        for epoch in range(epochs):\n",
    "            merged = input.to_frame().join(output)\n",
    "            merged = merged.sample(frac=batch_size_frac)\n",
    "            self.calculate_gradient(merged[\"x\"], merged[\"y\"])\n",
    "            self.apply_gradient(learning_rate)\n",
    "            mse = self.mean_squared_error(merged[\"x\"], merged[\"y\"])\n",
    "            mse_list.append(mse)\n",
    "            if epoch % 2000 == 0:\n",
    "                print(f\"Epoch {epoch} MSE: {mse}\")\n",
    "        return mse_list\n",
    "\n",
    "    def backpropagation(self, input, output):\n",
    "        \"\"\"\n",
    "        Perform backpropagation on the network\n",
    "        \"\"\"\n",
    "\n",
    "        for x,y in zip(input, output):\n",
    "            # Forward pass\n",
    "            a = x\n",
    "            for layer in self.layers:\n",
    "                z, a = layer.calculate_layer_and_before_activation(a)\n",
    "                layer.z = z\n",
    "                layer.a = a\n",
    "\n",
    "            # Backward pass\n",
    "            # Calculate the delta for the output layer\n",
    "            self.layers[-1].delta = self.mean_squared_error_gradient(\n",
    "                self.layers[-1].a, y\n",
    "            ) * self.layers[-1].activation_derivative(self.layers[-1].z)\n",
    "\n",
    "            # Calculate the delta for the hidden layers\n",
    "            for layer, next_layer in zip(self.layers[-2::-1], self.layers[-1::-1]):\n",
    "                layer.delta = np.dot(\n",
    "                    next_layer.delta, next_layer.weights.T\n",
    "                ) * layer.activation_derivative(layer.a)\n",
    "\n",
    "\n",
    "            # Calculate the gradients for the input layer\n",
    "            self.layers[0].biases_gradient += self.layers[0].delta\n",
    "            try:\n",
    "                self.layers[0].weights_gradient += np.dot(x.T, self.layers[0].delta)\n",
    "            except:\n",
    "                self.layers[0].weights_gradient += np.dot(x, self.layers[0].delta)\n",
    "\n",
    "            # Calculate the gradients for the hidden layers\n",
    "            for previous_layer, layer in zip(self.layers, self.layers[1:]):\n",
    "                layer.biases_gradient += layer.delta\n",
    "                layer.weights_gradient += np.dot(previous_layer.a.T, layer.delta)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.biases_gradient = layer.biases_gradient / len(input)\n",
    "            layer.weights_gradient = layer.weights_gradient / len(input)\n",
    "\n",
    "    def mean_squared_error(self, input, output):\n",
    "        \"\"\"\n",
    "        Calculate the mean squared error of the network on a given dataset and output\n",
    "        \"\"\"\n",
    "        mse = []\n",
    "        for i, j in zip(input, output):\n",
    "            mse.append((j - self.forward(i)) ** 2)\n",
    "        return np.mean(mse)\n",
    "\n",
    "    def mean_squared_error_gradient(self, predicted, true):\n",
    "        \"\"\"\n",
    "        Calculate the gradient of the mean squared error\n",
    "        \"\"\"\n",
    "        return 2 * (predicted - true)\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Visualize the network architecture\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(layer.report_layer(i))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    def apply_gradient(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.apply_gradient(learning_rate)\n",
    "\n",
    "    def apply_changes(self):\n",
    "        for layer in self.layers:\n",
    "            layer.apply_changes()\n",
    "\n",
    "    def calculate_momentum(self, momentum_decay):\n",
    "        for layer in self.layers:\n",
    "            layer.calculate_momentum(momentum_decay)\n",
    "\n",
    "    def calculate_gradient_squared(self, decay_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.calculate_gradient_squared(decay_rate)\n",
    "\n",
    "    def calculate_changes(self, learning_rate, epsilon, t, momentum_decay, decay_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.calculate_changes(learning_rate, epsilon, t, momentum_decay, decay_rate)\n",
    "\n",
    "    def calculate_gradient(self, input, output):\n",
    "        \"\"\"\n",
    "        Calculate the gradient of the network\n",
    "        \"\"\"\n",
    "        h = 0.0000001\n",
    "        original_mse = self.mean_squared_error(input, output)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            for i in range(layer.weights.shape[0]):\n",
    "                for j in range(layer.weights.shape[1]):\n",
    "                    layer.weights[i, j] += h\n",
    "                    new_mse = self.mean_squared_error(input, output)\n",
    "                    layer.weights_gradient[i, j] = (new_mse - original_mse) / h\n",
    "                    layer.weights[i, j] -= h\n",
    "\n",
    "            for i in range(layer.biases.shape[0]):\n",
    "                for j in range(layer.biases.shape[1]):\n",
    "                    layer.biases[i, j] += h\n",
    "                    new_mse = self.mean_squared_error(input, output)\n",
    "                    layer.biases_gradient[i, j] = (new_mse - original_mse) / h\n",
    "                    layer.biases[i, j] -= h\n",
    "\n",
    "    def adam(\n",
    "        self,\n",
    "        X,y,\n",
    "        learning_rate=0.01,\n",
    "        momentum_decay=0.9,\n",
    "        squared_gradient_decay=0.999,\n",
    "        max_num_epoch=1000,\n",
    "        batch_size=1,\n",
    "        batch_fraction=None,\n",
    "        epsilon=1e-8,\n",
    "        ):\n",
    "\n",
    "        # initialization\n",
    "        if type(X) is pd.DataFrame:\n",
    "            X = X.to_numpy()\n",
    "        if type(y) is pd.DataFrame:\n",
    "            y = y.to_numpy().T\n",
    "        counter = 0\n",
    "\n",
    "        # set batch size\n",
    "        assert type(batch_size) is int, \"batch_size must be an integer\"\n",
    "        if batch_fraction is not None:\n",
    "            assert 0 < batch_fraction <= 1, \"batch_fraction must be between 0 and 1\"\n",
    "            batch_size = int(X.shape[0] * batch_fraction)\n",
    "        iterations = int(X.shape[0] / batch_size)\n",
    "        mse_list = []\n",
    "\n",
    "        for i in range(max_num_epoch):\n",
    "            N = X.shape[0]\n",
    "            shuffled_idx = np.random.permutation(N)\n",
    "            # X, y = X[shuffled_idx], y[shuffled_idx]\n",
    "            for idx in range(iterations):\n",
    "                X_selected, y_selected = (\n",
    "                    X[idx * batch_size : (idx + 1) * batch_size],\n",
    "                    y[idx * batch_size : (idx + 1) * batch_size],\n",
    "                )\n",
    "                self.calculate_gradient(X_selected, y_selected)\n",
    "                self.calculate_momentum(momentum_decay)\n",
    "                self.calculate_gradient_squared(squared_gradient_decay)\n",
    "                counter += 1\n",
    "\n",
    "                self.calculate_changes(learning_rate, epsilon, counter, momentum_decay, squared_gradient_decay)\n",
    "                self.apply_changes()\n",
    "                mse_list.append(self.mean_squared_error(X_selected, y_selected))\n",
    "\n",
    "            print(\"Epoch:\", i)\n",
    "        return mse_list\n",
    "\n",
    "\n",
    "    def adam_debug(\n",
    "        self,\n",
    "        X,y,\n",
    "        learning_rate=0.01,\n",
    "        momentum_decay=0.9,\n",
    "        squared_gradient_decay=0.999,\n",
    "        max_num_epoch=1000,\n",
    "        batch_size=1,\n",
    "        batch_fraction=None,\n",
    "        epsilon=1e-8,\n",
    "        ):\n",
    "\n",
    "        # initialization\n",
    "        if type(X) is pd.DataFrame:\n",
    "            X = X.to_numpy()\n",
    "        if type(y) is pd.DataFrame:\n",
    "            y = y.to_numpy().T\n",
    "        counter = 0\n",
    "\n",
    "        # set batch size\n",
    "        assert type(batch_size) is int, \"batch_size must be an integer\"\n",
    "        if batch_fraction is not None:\n",
    "            assert 0 < batch_fraction <= 1, \"batch_fraction must be between 0 and 1\"\n",
    "            batch_size = int(X.shape[0] * batch_fraction)\n",
    "        iterations = int(X.shape[0] / batch_size)\n",
    "        mse_list = []\n",
    "\n",
    "        for i in range(max_num_epoch):\n",
    "            N = X.shape[0]\n",
    "            shuffled_idx = np.random.permutation(N)\n",
    "            # X, y = X[shuffled_idx], y[shuffled_idx]\n",
    "            for idx in range(iterations):\n",
    "                X_selected, y_selected = (\n",
    "                    X[idx * batch_size : (idx + 1) * batch_size],\n",
    "                    y[idx * batch_size : (idx + 1) * batch_size],\n",
    "                )\n",
    "                self.backpropagation(X_selected, y_selected)\n",
    "\n",
    "                self.calculate_momentum(momentum_decay)\n",
    "                self.calculate_gradient_squared(squared_gradient_decay)\n",
    "                counter += 1\n",
    "\n",
    "                self.calculate_changes(learning_rate, epsilon, counter, momentum_decay, squared_gradient_decay)\n",
    "                self.apply_changes()\n",
    "                mse_list.append(self.mean_squared_error(X_selected, y_selected))\n",
    "\n",
    "            print(\"Epoch:\", i)\n",
    "        return mse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_simple_train = pd.read_csv(\"../data/regression/square-simple-training.csv\", index_col=0)\n",
    "square_simple_test = pd.read_csv(\"../data/regression/square-simple-test.csv\", index_col=0)\n",
    "plt.scatter(square_simple_train[\"x\"], square_simple_train[\"y\"])\n",
    "square_simple_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural = NeuralNetwork([5], 1, 1)\n",
    "neural.mean_squared_error(square_simple_train[\"x\"], square_simple_train[\"y\"])\n",
    "mse = neural.train(square_simple_train[\"x\"], square_simple_train[\"y\"], learning_rate=0.001, epochs=16000, batch_size_frac=1)\n",
    "neural.mean_squared_error(square_simple_test[\"x\"], square_simple_test[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse)\n",
    "plt.title(\"Mean Squared Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in square_simple_test[\"x\"]:\n",
    "    y.append(neural.forward(i))\n",
    "plt.scatter(square_simple_test[\"x\"], y, c=\"red\")\n",
    "plt.scatter(square_simple_test[\"x\"], square_simple_test[\"y\"], c=\"blue\")\n",
    "plt.legend([\"Prediction\", \"True\"])\n",
    "neural.mean_squared_error(square_simple_test[\"x\"], square_simple_test[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = square_simple_train.mean()\n",
    "std = square_simple_train.std()\n",
    "square_simple_train_normalized = (square_simple_train - mean) / std\n",
    "square_simple_test_normalized = (square_simple_test - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural = NeuralNetwork([5], 1, 1)\n",
    "neural.mean_squared_error(\n",
    "    square_simple_train_normalized[\"x\"], square_simple_train_normalized[\"y\"]\n",
    ")\n",
    "mse = neural.train(\n",
    "    square_simple_train_normalized[\"x\"],\n",
    "    square_simple_train_normalized[\"y\"],\n",
    "    learning_rate=0.1,\n",
    "    epochs=8000,\n",
    "    batch_size_frac=0.2,\n",
    ")\n",
    "neural.mean_squared_error(\n",
    "    square_simple_train_normalized[\"x\"], square_simple_train_normalized[\"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse)\n",
    "plt.title(\"Mean Squared Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in square_simple_train_normalized[\"x\"]:\n",
    "    y.append(neural.forward(i))\n",
    "plt.scatter(square_simple_train_normalized[\"x\"], y, c=\"red\")\n",
    "plt.scatter(\n",
    "    square_simple_train_normalized[\"x\"],\n",
    "    square_simple_train_normalized[\"y\"],\n",
    "    c=\"blue\",\n",
    ")\n",
    "plt.legend([\"Prediction\", \"True\"])\n",
    "neural.mean_squared_error(\n",
    "    square_simple_train_normalized[\"x\"], square_simple_train_normalized[\"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = 0\n",
    "for x,y in zip(square_simple_test_normalized[\"x\"], square_simple_test_normalized[\"y\"]):\n",
    "    val = neural.forward(x)\n",
    "    val = val * std[\"y\"] + mean[\"y\"]\n",
    "    y = y * std[\"y\"] + mean[\"y\"]\n",
    "    mse += (val - y) ** 2\n",
    "print(mse / len(square_simple_test_normalized))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_large_train = pd.read_csv(\"../data/regression/steps-large-test.csv\", index_col=0)\n",
    "steps_large_test = pd.read_csv(\"../data/regression/steps-large-test.csv\", index_col=0)\n",
    "plt.scatter(steps_large_train[\"x\"], steps_large_train[\"y\"])\n",
    "steps_large_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = steps_large_train.mean()\n",
    "std = steps_large_train.std()\n",
    "steps_large_train_normalized = (steps_large_train - mean) / std\n",
    "steps_large_test_normalized = (steps_large_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural = NeuralNetwork([5,10], 1, 1)\n",
    "neural.mean_squared_error(\n",
    "    steps_large_train[\"x\"], steps_large_train[\"y\"]\n",
    ")\n",
    "mse = neural.train(\n",
    "    steps_large_train[\"x\"],\n",
    "    steps_large_train[\"y\"],\n",
    "    learning_rate=0.001,\n",
    "    epochs=1500,\n",
    "    batch_size_frac=0.2,\n",
    ")\n",
    "neural.mean_squared_error(\n",
    "    steps_large_train[\"x\"], steps_large_train[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse)\n",
    "plt.title(\"Mean Squared Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in steps_large_test[\"x\"]:\n",
    "    y.append(neural.forward(i))\n",
    "plt.scatter(steps_large_test[\"x\"], y, c=\"red\")\n",
    "plt.scatter(\n",
    "    steps_large_test[\"x\"],\n",
    "    steps_large_test[\"y\"],\n",
    "    c=\"blue\",\n",
    ")\n",
    "plt.legend([\"Prediction\", \"True\"])\n",
    "neural.mean_squared_error(\n",
    "    steps_large_test[\"x\"], steps_large_test[\"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = 0\n",
    "for x, y in zip(steps_large_test_normalized[\"x\"], steps_large_test_normalized[\"y\"]):\n",
    "    val = neural.forward(x)\n",
    "    val = val * std[\"y\"] + mean[\"y\"]\n",
    "    y = y * std[\"y\"] + mean[\"y\"]\n",
    "    mse += (val - y) ** 2\n",
    "print(mse / len(steps_large_test_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural = NeuralNetwork([5], 1, 1)\n",
    "neural.mean_squared_error(\n",
    "    square_simple_train_normalized[\"x\"], square_simple_train_normalized[\"y\"]\n",
    ")\n",
    "mse = neural.adam(\n",
    "    square_simple_train_normalized[\"x\"],\n",
    "    square_simple_train_normalized[\"y\"],\n",
    "    max_num_epoch=100,\n",
    "    learning_rate=0.1,\n",
    "    batch_fraction=1,\n",
    ")\n",
    "neural.mean_squared_error(\n",
    "    square_simple_train_normalized[\"x\"], square_simple_train_normalized[\"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_large = pd.read_csv(\"../data/regression/multimodal-large-test.csv\")\n",
    "multimodal_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = multimodal_large.mean()\n",
    "std = multimodal_large.std()\n",
    "multimodal_large_normalized = (multimodal_large - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural = NeuralNetwork([10], 1, 1)\n",
    "neural.mean_squared_error(\n",
    "    multimodal_large_normalized[\"x\"],\n",
    "    multimodal_large_normalized[\"y\"],\n",
    ")\n",
    "mse = neural.adam_debug(\n",
    "    multimodal_large_normalized[\"x\"],\n",
    "    multimodal_large_normalized[\"y\"],\n",
    "    batch_size=100,\n",
    "    max_num_epoch=2000,\n",
    "    learning_rate=0.05,\n",
    ")\n",
    "neural.mean_squared_error(\n",
    "    multimodal_large_normalized[\"x\"],\n",
    "    multimodal_large_normalized[\"y\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse)\n",
    "plt.title(\"Mean Squared Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in multimodal_large_normalized[\"x\"]:\n",
    "    y.append(neural.forward(i))\n",
    "plt.scatter(multimodal_large_normalized[\"x\"], y, c=\"red\")\n",
    "plt.scatter(\n",
    "    multimodal_large_normalized[\"x\"],\n",
    "    multimodal_large_normalized[\"y\"],\n",
    "    c=\"blue\",\n",
    ")\n",
    "plt.legend([\"Prediction\", \"True\"])\n",
    "neural.mean_squared_error(\n",
    "    multimodal_large_normalized[\"x\"], multimodal_large_normalized[\"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(numpy_matrix)\n",
    "np.random.shuffle(numpy_matrix)\n",
    "print(numpy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asseco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

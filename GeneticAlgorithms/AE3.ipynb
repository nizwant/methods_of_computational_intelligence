{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from abc import ABC, abstractmethod\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Initialization(ABC):\n",
    "    @abstractmethod\n",
    "    def initialize(self, n_output: int, n_input: int):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NormalXavier(Initialization):\n",
    "    def initialize(self, n_output: int, n_input: int):\n",
    "        std = np.sqrt(2 / (n_output + n_input))\n",
    "        return np.random.normal(0, std, (n_output, n_input))\n",
    "\n",
    "\n",
    "class UniformXavier(Initialization):\n",
    "    def initialize(self, n_output: int, n_input: int):\n",
    "        val = np.sqrt(6 / (n_output + n_input))\n",
    "        return np.random.uniform(-val, val, (n_output, n_input))\n",
    "\n",
    "\n",
    "class He(Initialization):\n",
    "    def initialize(self, n_output: int, n_input: int):\n",
    "        return np.random.normal(0, np.sqrt(2 / n_output), (n_output, n_input))\n",
    "\n",
    "\n",
    "class Zero(Initialization):\n",
    "    def initialize(self, n_output: int, n_input: int):\n",
    "        return np.zeros((n_output, n_input))\n",
    "\n",
    "\n",
    "class Normal(Initialization):\n",
    "    def initialize(self, n_output: int, n_input: int):\n",
    "        return np.random.normal(0, 1, (n_output, n_input))\n",
    "\n",
    "\n",
    "class Uniform_minus_one_one(Initialization):\n",
    "    def initialize(self, n_output: int, n_input: int):\n",
    "        return np.random.uniform(-1, 1, (n_output, n_input))\n",
    "\n",
    "\n",
    "class Uniform_zero_one(Initialization):\n",
    "    def initialize(self, n_output: int, n_input: int):\n",
    "        return np.random.uniform(0, 1, (n_output, n_input))\n",
    "\n",
    "\n",
    "class InitializationBuilder:\n",
    "    @staticmethod\n",
    "    def get_initialization(initialization: str, nodes_out: int, nodes_in: int):\n",
    "        if initialization == \"he\":\n",
    "            return He().initialize(nodes_out, nodes_in)\n",
    "        elif initialization == \"normal\":\n",
    "            return Normal().initialize(nodes_out, nodes_in)\n",
    "        elif initialization == \"normal_xavier\":\n",
    "            return NormalXavier().initialize(nodes_out, nodes_in)\n",
    "        elif initialization == \"uniform_xavier\":\n",
    "            return UniformXavier().initialize(nodes_out, nodes_in)\n",
    "        elif initialization == \"uniform_minus_one_one\":\n",
    "            return Uniform_minus_one_one().initialize(nodes_out, nodes_in)\n",
    "        elif initialization == \"uniform_zero_one\":\n",
    "            return Uniform_zero_one().initialize(nodes_out, nodes_in)\n",
    "        elif initialization == \"zero\":\n",
    "            return Zero().initialize(nodes_out, nodes_in)\n",
    "        else:\n",
    "            raise ValueError(f\"Initialization function {initialization} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(ABC):\n",
    "    @abstractmethod\n",
    "    def activation(self, x: np.double) -> np.double:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def activation(self, x: np.double) -> np.double:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "    def activation(self, x: np.double) -> np.double:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "    def activation(self, x: np.double) -> np.double:\n",
    "        return np.tanh(x)\n",
    "\n",
    "\n",
    "class Softmax(Activation):\n",
    "    \"\"\"\n",
    "    Softmax function is used in the output layer of a neural network for multi-class classification problems.\n",
    "    It squashes the output of each unit to be between 0 and 1, just like a sigmoid function.\n",
    "    It also divides each output such that the total sum of the outputs is equal to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def activation(self, x: np.double) -> np.double:\n",
    "        if x.ndim == 1:\n",
    "            exps = np.exp(x - np.max(x))\n",
    "            return exps / np.sum(exps)\n",
    "        exps = np.exp(x - x.max(axis=0, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "class Linear(Activation):\n",
    "    def activation(self, x: np.double) -> np.double:\n",
    "        return x\n",
    "\n",
    "\n",
    "class LeakyReLU(Activation):\n",
    "    def activation(self, x: np.double) -> np.double:\n",
    "        return np.maximum(0.01 * x, x)\n",
    "\n",
    "\n",
    "class ActivationBuilder:\n",
    "    @staticmethod\n",
    "    def get_activation(activation: str):\n",
    "        if activation == \"sigmoid\":\n",
    "            return Sigmoid()\n",
    "        elif activation == \"relu\":\n",
    "            return ReLU()\n",
    "        elif activation == \"tanh\":\n",
    "            return Tanh()\n",
    "        elif activation == \"softmax\":\n",
    "            return Softmax()\n",
    "        elif activation == \"linear\":\n",
    "            return Linear()\n",
    "        elif activation == \"leaky_relu\":\n",
    "            return LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function {activation} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def cost(self, y, y_hat):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MeanSquaredError(CostFunction):\n",
    "    def cost(self, y_hat: np.ndarray, y: np.ndarray) -> float:\n",
    "        if isinstance(y, pd.Series) or isinstance(y, pd.DataFrame):\n",
    "            y = y.to_numpy()\n",
    "            if len(y.shape) == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "        return np.mean((y - y_hat) ** 2)\n",
    "\n",
    "\n",
    "class AbsoluteError(CostFunction):\n",
    "    def cost(self, y_hat: np.ndarray, y: np.ndarray) -> float:\n",
    "        if isinstance(y, pd.Series) or isinstance(y, pd.DataFrame):\n",
    "            y = y.to_numpy()\n",
    "            if len(y.shape) == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "        return np.mean(np.abs(y - y_hat))\n",
    "\n",
    "\n",
    "class CrossEntropyWithSoftmax(CostFunction):\n",
    "    EPSILON = 1e-10\n",
    "\n",
    "    def cost(self, y_hat: np.ndarray, y: np.ndarray) -> float:\n",
    "        y_hat = np.clip(y_hat, self.EPSILON, 1)\n",
    "        return -np.mean(y * np.log(y_hat))\n",
    "\n",
    "\n",
    "class CostFunctionBuilder:\n",
    "    def build_cost_function(self, cost_function: str):\n",
    "        if cost_function == \"mse\":\n",
    "            return MeanSquaredError()\n",
    "        elif cost_function == \"ae\":\n",
    "            return AbsoluteError()\n",
    "        elif cost_function == \"cross_entropy_with_softmax\":\n",
    "            return CrossEntropyWithSoftmax()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid cost function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    __slots__ = [\n",
    "        \"nodes_in\",\n",
    "        \"nodes_out\",\n",
    "        \"weights\",\n",
    "        \"biases\",\n",
    "        \"activation\",\n",
    "        \"a\",\n",
    "        \"z\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nodes_in,\n",
    "        nodes_out,\n",
    "        activation=\"sigmoid\",\n",
    "        weight_initialization=\"he\",\n",
    "        bias_initialization=\"zero\",\n",
    "    ):\n",
    "\n",
    "        self.nodes_in = nodes_in\n",
    "        self.nodes_out = nodes_out\n",
    "\n",
    "        self.weights = InitializationBuilder.get_initialization(\n",
    "            weight_initialization, nodes_out, nodes_in\n",
    "        )\n",
    "\n",
    "        self.biases = InitializationBuilder.get_initialization(\n",
    "            bias_initialization, nodes_out, 1\n",
    "        )\n",
    "        self.activation = ActivationBuilder.get_activation(activation)\n",
    "\n",
    "    def forward(self, a):\n",
    "        self.z = np.dot(self.weights, a) + self.biases\n",
    "        self.a = self.activation.activation(self.z)\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    __slots__ = [\n",
    "        \"layers\",\n",
    "        \"optimizer\",\n",
    "        \"cost_function\",\n",
    "        \"layer_sizes\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_function=\"mse\",\n",
    "    ):\n",
    "        self.layers = []\n",
    "        self.layer_sizes = []\n",
    "        self.cost_function = CostFunctionBuilder().build_cost_function(cost_function)\n",
    "\n",
    "    def add_layer(self, layer: Layer):\n",
    "        if not self.layer_sizes:\n",
    "            self.layer_sizes = [layer.nodes_in]\n",
    "        else:\n",
    "            assert (\n",
    "                layer.nodes_in == self.layer_sizes[-1]\n",
    "            ), f\"Output in previous layer doesn't match input in this layer\"\n",
    "        self.layer_sizes.append(layer.nodes_out)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def _forward(self, x: np.ndarray):\n",
    "        if isinstance(x, pd.DataFrame) or isinstance(x, pd.Series):\n",
    "            x = x.to_numpy()\n",
    "            if len(x.shape) == 1:\n",
    "                x = x.reshape(-1, 1)\n",
    "        a = x.T\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        return self._forward(x).T\n",
    "\n",
    "    def predict_class(self, x: np.ndarray):\n",
    "        return np.argmax(self.predict(x), axis=1, keepdims=True)\n",
    "\n",
    "    def flatten_weights_and_biases(self):\n",
    "        weights_and_biases = []\n",
    "        for layer in self.layers:\n",
    "            weights_and_biases.append(layer.weights.flatten())\n",
    "            weights_and_biases.append(layer.biases.flatten())\n",
    "        return np.concatenate(weights_and_biases)\n",
    "\n",
    "    def deflatten_weights_and_biases(self, solution):\n",
    "        for layer in self.layers:\n",
    "            layer.weights = solution[: layer.weights.size].reshape(layer.weights.shape)\n",
    "            solution = solution[layer.weights.size :]\n",
    "            layer.biases = solution[: layer.biases.size].reshape(layer.biases.shape)\n",
    "            solution = solution[layer.biases.size :]\n",
    "\n",
    "    def calculate_cost(self, x: np.ndarray, y: np.ndarray):\n",
    "        base_cost = self.cost_function.cost(self.predict(x), y)\n",
    "        return base_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof that it works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2534341281830511"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(cost_function=\"mse\")\n",
    "nn.add_layer(Layer(2, 3, activation=\"relu\"))\n",
    "nn.add_layer(Layer(3, 3, activation=\"relu\"))\n",
    "nn.add_layer(Layer(3, 1, activation=\"sigmoid\"))\n",
    "\n",
    "# create dummy data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)\n",
    "y = np.random.randint(0, 2, (100, 1))\n",
    "\n",
    "# train the model\n",
    "nn.calculate_cost(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evolutionary_Algorithm:\n",
    "    __slots__ = [\n",
    "        \"mutation_rate\",\n",
    "        \"crossover_rate\",\n",
    "        \"number_of_generations\",\n",
    "        \"population_size\",\n",
    "        \"population\",\n",
    "        \"problem_dim\",\n",
    "        \"hall_of_fame\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mutation_rate=0.7,\n",
    "        crossover_rate=0.7,\n",
    "        number_of_generations=50,\n",
    "        population_size=100,\n",
    "    ):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_generations = number_of_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.population = None\n",
    "        self.problem_dim = None\n",
    "        self.hall_of_fame = []\n",
    "\n",
    "    def _generate_population(\n",
    "        self,\n",
    "        network_shape,\n",
    "        cost_function=\"mse\",\n",
    "        activation=\"relu\",\n",
    "        last_activation=\"sigmoid\",\n",
    "    ):\n",
    "        self.population = []\n",
    "        self.problem_dim = sum(\n",
    "            [\n",
    "                layer.nodes_in * layer.nodes_out + layer.nodes_out\n",
    "                for layer in network_shape\n",
    "            ]\n",
    "        )\n",
    "        for _ in range(self.population_size):\n",
    "            nn = NeuralNetwork(cost_function=cost_function)\n",
    "            for input_size, output_size in zip(network_shape[:-1], network_shape[1:-1]):\n",
    "                nn.add_layer(Layer(input_size, output_size, activation=activation))\n",
    "            nn.add_layer(\n",
    "                Layer(network_shape[-2], network_shape[-1], activation=last_activation)\n",
    "            )\n",
    "            self.population.append(nn.flatten_weights_and_biases())\n",
    "\n",
    "    def _mutation(self, individual):\n",
    "        if np.random.rand() < self.mutation_rate:\n",
    "            new_individual = individual.deepcopy()\n",
    "            weights_and_biases = new_individual.flatten_weights_and_biases()\n",
    "            weights_and_biases += np.random.normal(0, 0.01, self.problem_dim)\n",
    "            new_individual.deflatten_weights_and_biases(weights_and_biases)\n",
    "        return new_individual\n",
    "\n",
    "    def _crossover(self, parent1, parent2):\n",
    "        if np.random.rand() < self.crossover_rate:\n",
    "            crossover_point = np.random.randint(1, self.problem_dim - 1)\n",
    "            child1 = parent1.deepcopy()\n",
    "            child2 = parent2.deepcopy()\n",
    "            weights_and_biases_child1 = child1.flatten_weights_and_biases().copy()\n",
    "            weights_and_biases_child2 = child2.flatten_weights_and_biases().copy()\n",
    "\n",
    "            child1.deflatten_weights_and_biases(\n",
    "                np.concatenate(\n",
    "                    (\n",
    "                        weights_and_biases_child1[:crossover_point],\n",
    "                        weights_and_biases_child2[crossover_point:],\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            child2.deflatten_weights_and_biases(\n",
    "                np.concatenate(\n",
    "                    (\n",
    "                        weights_and_biases_child2[:crossover_point],\n",
    "                        weights_and_biases_child1[crossover_point:],\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            return child1, child2\n",
    "        return parent1, parent2\n",
    "\n",
    "    def _evaluate_population(self):\n",
    "        fitness = np.zeros(len(self.population))\n",
    "        for i, individual in enumerate(self.population):\n",
    "            fitness[i] = self.function_to_optimize(*individual)\n",
    "        self.hall_of_fame.append(np.argsort(fitness)[: int(self.population_size * 0.1)])\n",
    "        return fitness\n",
    "\n",
    "    def _tournament_selection(self, fitness):\n",
    "        fitness = 1 / fitness\n",
    "        probabilities = fitness / np.sum(fitness)\n",
    "        return self.population[\n",
    "            np.random.choice(\n",
    "                range(len(self.population)),\n",
    "                p=probabilities,\n",
    "                size=self.population_size - len(self.hall_of_fame[-1]),\n",
    "                replace=True,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def _select_individual(self, n=1):\n",
    "        return self.population[\n",
    "            np.random.choice(range(self.population_size), size=n, replace=False)\n",
    "        ]\n",
    "\n",
    "    def _visualize_individual(self, individual):\n",
    "        print(individual)\n",
    "\n",
    "    def optimize(self, silent=False):\n",
    "        # initialize the population\n",
    "        self._generate_population(self.problem_dim)\n",
    "        best_evaluation_in_iteration = []\n",
    "        best_solution_in_iteration = []\n",
    "\n",
    "        # main algorithm loop\n",
    "        for generation in range(self.number_of_generations):\n",
    "\n",
    "            # crossover\n",
    "            children = np.zeros(\n",
    "                (2 * len(range(0, self.population_size, 2)), self.problem_dim)\n",
    "            )\n",
    "            for i in range(0, self.population_size, 2):\n",
    "                parent1, parent2 = self._select_individual(n=2)\n",
    "                children[i], children[i + 1] = self._crossover(parent1, parent2)\n",
    "            self.population = np.vstack([self.population, children])\n",
    "\n",
    "            # mutation\n",
    "            # we mutate every individual in the population not random one\n",
    "            mutated = np.zeros((self.population_size, self.problem_dim))\n",
    "            for i in range(self.population_size):\n",
    "                individual = self._select_individual()\n",
    "                mutated[i] = self._mutation(individual)\n",
    "            self.population = np.vstack([self.population, mutated])\n",
    "\n",
    "            # evaluate the population and log the best solution\n",
    "            fitness = self._evaluate_population()\n",
    "            if not silent:\n",
    "                print(\n",
    "                    f\"iter: {generation}, best: {np.min(fitness)} for {['%.3f' % n for n in self.population[self.hall_of_fame[-1][0]]]}\"\n",
    "                )\n",
    "\n",
    "            best_evaluation_in_iteration.append(np.min(fitness))\n",
    "            best_solution_in_iteration.append(self.population[self.hall_of_fame[-1][0]])\n",
    "\n",
    "            # create new population\n",
    "            self.population = np.vstack(\n",
    "                [\n",
    "                    self.population[self.hall_of_fame[-1]],\n",
    "                    self._tournament_selection(fitness),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return best_evaluation_in_iteration, best_solution_in_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "auto_mpg = fetch_ucirepo(id=9)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = auto_mpg.data.features\n",
    "y = auto_mpg.data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "iris = fetch_ucirepo(id=53)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = iris.data.features\n",
    "y = iris.data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_large_train = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/nizwant/miowid/main/data/regression/multimodal-large-training.csv\"\n",
    ")\n",
    "multimodal_large_test = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/nizwant/miowid/main/data/regression/multimodal-large-test.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = multimodal_large_train.mean()\n",
    "std = multimodal_large_train.std()\n",
    "multimodal_large_train = (multimodal_large_train - mean) / std\n",
    "multimodal_large_test = (multimodal_large_test - mean) / std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algorytmyistdanych",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

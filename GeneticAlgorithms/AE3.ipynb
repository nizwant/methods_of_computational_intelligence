{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /Users/mat/opt/anaconda3/envs/asseco/lib/python3.10/site-packages (0.0.3)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from initialization_builder import InitializationBuilder\n",
    "from activation_builder import ActivationBuilder\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    __slots__ = [\n",
    "        \"nodes_in\",\n",
    "        \"nodes_out\",\n",
    "        \"weights\",\n",
    "        \"weights_gradient\",\n",
    "        \"biases\",\n",
    "        \"biases_gradient\",\n",
    "        \"activation\",\n",
    "        \"a\",\n",
    "        \"z\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nodes_in,\n",
    "        nodes_out,\n",
    "        activation=\"sigmoid\",\n",
    "        weight_initialization=\"he\",\n",
    "        bias_initialization=\"zero\",\n",
    "    ):\n",
    "\n",
    "        self.nodes_in = nodes_in\n",
    "        self.nodes_out = nodes_out\n",
    "\n",
    "        self.weights = InitializationBuilder.get_initialization(\n",
    "            weight_initialization, nodes_out, nodes_in\n",
    "        )\n",
    "\n",
    "        self.weights_gradient = InitializationBuilder.get_initialization(\n",
    "            \"zero\", nodes_out, nodes_in\n",
    "        )\n",
    "\n",
    "        self.biases = InitializationBuilder.get_initialization(\n",
    "            bias_initialization, nodes_out, 1\n",
    "        )\n",
    "        self.biases_gradient = InitializationBuilder.get_initialization(\n",
    "            \"zero\", nodes_out, 1\n",
    "        )\n",
    "\n",
    "        self.activation = ActivationBuilder.get_activation(activation)\n",
    "\n",
    "    def forward(self, a):\n",
    "        self.z = np.dot(self.weights, a) + self.biases\n",
    "        self.a = self.activation.activation(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def report_layer(self, layer_num):\n",
    "        return (\n",
    "            f\"Layer number {layer_num}\\nWeights\\n{self.weights}\\nbiases\\n{self.biases}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from layer_v2 import Layer\n",
    "from optimizers_builder import OptimizersBuilder\n",
    "from cost_function_builder import CostFunctionBuilder\n",
    "from helper_functions import HelperFunction\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    __slots__ = [\n",
    "        \"layers\",\n",
    "        \"optimizer\",\n",
    "        \"cost_function\",\n",
    "        \"layer_sizes\",\n",
    "        \"regularization\",\n",
    "        \"C\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self, optimizer=\"adam\", cost_function=\"mse\", regularization=None, C=0.01\n",
    "    ):\n",
    "        self.layers = []\n",
    "        self.layer_sizes = []\n",
    "        self.optimizer = OptimizersBuilder().build_optimizer(optimizer)\n",
    "        self.cost_function = CostFunctionBuilder().build_cost_function(cost_function)\n",
    "        assert regularization in [None, \"l1\", \"l2\"], \"Regularization not supported\"\n",
    "        self.regularization = regularization\n",
    "        self.C = C  # strength of regularization\n",
    "\n",
    "    def add_layer(self, layer: Layer):\n",
    "        if not self.layer_sizes:\n",
    "            self.layer_sizes = [layer.nodes_in]\n",
    "        else:\n",
    "            assert (\n",
    "                layer.nodes_in == self.layer_sizes[-1]\n",
    "            ), f\"Output in previous layer doesn't match input in this layer\"\n",
    "        self.layer_sizes.append(layer.nodes_out)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def _forward(self, x: np.ndarray):\n",
    "        if isinstance(x, pd.DataFrame) or isinstance(x, pd.Series):\n",
    "            x = x.to_numpy()\n",
    "            if len(x.shape) == 1:\n",
    "                x = x.reshape(-1, 1)\n",
    "        a = x.T\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        return self._forward(x).T\n",
    "\n",
    "    def predict_class(self, x: np.ndarray):\n",
    "        return np.argmax(self.predict(x), axis=1, keepdims=True)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            print(layer.report_layer(i))\n",
    "        return \"\"\n",
    "\n",
    "    def flatten_weights_and_biases(self):\n",
    "        weights_and_biases = []\n",
    "        for layer in self.layers:\n",
    "            weights_and_biases.append(layer.weights.flatten())\n",
    "            weights_and_biases.append(layer.biases.flatten())\n",
    "        return np.concatenate(weights_and_biases)\n",
    "\n",
    "    def deflatten_weights_and_biases(self, solution):\n",
    "        for layer in self.layers:\n",
    "            layer.weights = solution[: layer.weights.size].reshape(layer.weights.shape)\n",
    "            solution = solution[layer.weights.size :]\n",
    "            layer.biases = solution[: layer.biases.size].reshape(layer.biases.shape)\n",
    "            solution = solution[layer.biases.size :]\n",
    "\n",
    "    def flatted_gradient(self):\n",
    "        gradients = []\n",
    "        for layer in self.layers:\n",
    "            gradients.append(layer.weights_gradient.flatten())\n",
    "            gradients.append(layer.biases_gradient.flatten())\n",
    "        return np.concatenate(gradients)\n",
    "\n",
    "    def calculate_gradient_numerically(self, x: np.ndarray, y: np.ndarray, h=1e-5):\n",
    "        \"\"\"\n",
    "        THIS FUNCTION IS ONLY FOR EXPERIMENTAL PURPOSES\n",
    "        It calculates the gradient numerically.\n",
    "        It is to slow to be used in practice\n",
    "        \"\"\"\n",
    "        initial_cost = self.cost_function.cost(self.predict(x), y)\n",
    "        for layer in self.layers:\n",
    "            for i in range(layer.weights.shape[0]):\n",
    "                for j in range(layer.weights.shape[1]):\n",
    "                    layer.weights[i, j] += h\n",
    "                    new_cost = self.cost_function.cost(self.predict(x), y)\n",
    "                    layer.weights[i, j] -= h\n",
    "\n",
    "                    layer.weights_gradient[i, j] = (new_cost - initial_cost) / h\n",
    "\n",
    "            for i in range(layer.biases.shape[0]):\n",
    "                for j in range(layer.biases.shape[1]):\n",
    "                    layer.biases[i, j] += h\n",
    "                    new_cost = self.cost_function.cost(self.predict(x), y)\n",
    "                    layer.biases[i, j] -= h\n",
    "                    layer.biases_gradient[i, j] = (new_cost - initial_cost) / h\n",
    "\n",
    "    def backpropagation(self, x: np.ndarray, y: np.ndarray):\n",
    "        a = self._forward(x)\n",
    "        y = y.T\n",
    "        delta = self.cost_function.cost_derivative(a, y) * self.layers[\n",
    "            -1\n",
    "        ].activation.derivative(self.layers[-1].z)\n",
    "\n",
    "        # Calculate gradients for the last layer\n",
    "        self.layers[-1].biases_gradient = np.mean(delta, axis=1, keepdims=True)\n",
    "        self.layers[-1].weights_gradient = (\n",
    "            np.dot(delta, self.layers[-2].a.T) / x.shape[0]\n",
    "        )\n",
    "\n",
    "        # Add regularization to the last layer\n",
    "        if self.regularization == \"l1\":\n",
    "            self.layers[-1].weights_gradient += (\n",
    "                self.C * np.sign(self.layers[-1].weights) / x.shape[0]\n",
    "            )\n",
    "        elif self.regularization == \"l2\":\n",
    "            self.layers[-1].weights_gradient += (\n",
    "                2 * self.C * self.layers[-1].weights / x.shape[0]\n",
    "            )\n",
    "\n",
    "        for previous_layer, layer, next_layer in zip(\n",
    "            self.layers[-3::-1], self.layers[-2::-1], self.layers[::-1]\n",
    "        ):\n",
    "            delta = np.dot(next_layer.weights.T, delta) * layer.activation.derivative(\n",
    "                layer.z\n",
    "            )\n",
    "\n",
    "            # Calculate gradients for the all but first hidden layer\n",
    "            layer.biases_gradient = np.mean(delta, axis=1, keepdims=True)\n",
    "            layer.weights_gradient = np.dot(delta, previous_layer.a.T) / x.shape[0]\n",
    "\n",
    "            # Add regularization to the layer\n",
    "            if self.regularization == \"l1\":\n",
    "                layer.weights_gradient += self.C * np.sign(layer.weights) / x.shape[0]\n",
    "            elif self.regularization == \"l2\":\n",
    "                layer.weights_gradient += 2 * self.C * layer.weights / x.shape[0]\n",
    "\n",
    "        delta = np.dot(self.layers[1].weights.T, delta) * self.layers[\n",
    "            0\n",
    "        ].activation.derivative(self.layers[0].z)\n",
    "\n",
    "        # Calculate gradients for the first hidden layer\n",
    "        self.layers[0].biases_gradient = np.mean(delta, axis=1, keepdims=True)\n",
    "        self.layers[0].weights_gradient = np.dot(delta, x) / x.shape[0]\n",
    "\n",
    "        # Add regularization to the first hidden layer\n",
    "        if self.regularization == \"l1\":\n",
    "            self.layers[0].weights_gradient += (\n",
    "                self.C * np.sign(self.layers[0].weights) / x.shape[0]\n",
    "            )\n",
    "        elif self.regularization == \"l2\":\n",
    "            self.layers[0].weights_gradient += (\n",
    "                2 * self.C * self.layers[0].weights / x.shape[0]\n",
    "            )\n",
    "\n",
    "    def calculate_and_extract_gradient(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        current_solution: np.ndarray,\n",
    "        use_backpropagation=True,\n",
    "    ):\n",
    "        self.deflatten_weights_and_biases(current_solution)\n",
    "        if use_backpropagation:\n",
    "            self.backpropagation(x, y)\n",
    "        else:\n",
    "            self.calculate_gradient_numerically(x, y)\n",
    "        return self.flatted_gradient()\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        learning_rate=0.01,\n",
    "        max_num_epoch=1000,\n",
    "        batch_size=30,\n",
    "        batch_fraction=None,\n",
    "        using_backpropagation=True,\n",
    "        silent=True,\n",
    "    ):\n",
    "        mse_after_epoch_train = self.optimizer(\n",
    "            X=X,\n",
    "            y=y,\n",
    "            using_backpropagation=using_backpropagation,\n",
    "            learning_rate=learning_rate,\n",
    "            max_num_epoch=max_num_epoch,\n",
    "            batch_size=batch_size,\n",
    "            batch_fraction=batch_fraction,\n",
    "            neural_network=self,\n",
    "            silent=silent,\n",
    "        )\n",
    "        return mse_after_epoch_train\n",
    "\n",
    "    def train_with_early_stopping(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        learning_rate=0.01,\n",
    "        max_num_epoch=1000,\n",
    "        batch_size=30,\n",
    "        batch_fraction=None,\n",
    "        using_backpropagation=True,\n",
    "        silent=True,\n",
    "    ):\n",
    "        mse_after_epoch_train, mse_after_epoch_test = self.optimizer(\n",
    "            X=X,\n",
    "            y=y,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            using_backpropagation=using_backpropagation,\n",
    "            learning_rate=learning_rate,\n",
    "            max_num_epoch=max_num_epoch,\n",
    "            batch_size=batch_size,\n",
    "            batch_fraction=batch_fraction,\n",
    "            neural_network=self,\n",
    "            silent=silent,\n",
    "        )\n",
    "        return mse_after_epoch_train, mse_after_epoch_test\n",
    "\n",
    "    def calculate_cost(self, x: np.ndarray, y: np.ndarray):\n",
    "        base_cost = self.cost_function.cost(self.predict(x), y)\n",
    "        if self.regularization is None:\n",
    "            return base_cost\n",
    "        if self.regularization == \"l1\":\n",
    "            return base_cost + self.C * sum(\n",
    "                np.sum(np.abs(layer.weights)) for layer in self.layers\n",
    "            )\n",
    "        if self.regularization == \"l2\":\n",
    "            return base_cost + self.C * sum(\n",
    "                np.sum(layer.weights**2) for layer in self.layers\n",
    "            )\n",
    "\n",
    "    def visualize_network(self):\n",
    "        HelperFunction.visualize_network(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evolutionary_Algorithm:\n",
    "    __slots__ = [\n",
    "        \"mutation_rate\",\n",
    "        \"crossover_rate\",\n",
    "        \"number_of_generations\",\n",
    "        \"population_size\",\n",
    "        \"function_to_optimize\",\n",
    "        \"population\",\n",
    "        \"problem_dim\",\n",
    "        \"hall_of_fame\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mutation_rate=0.7,\n",
    "        crossover_rate=0.7,\n",
    "        number_of_generations=50,\n",
    "        population_size=100,\n",
    "    ):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_generations = number_of_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.function_to_optimize = None\n",
    "        self.population = None\n",
    "        self.problem_dim = None\n",
    "        self.hall_of_fame = []\n",
    "\n",
    "    def _generate_population(self, n_genes):\n",
    "        self.population = np.random.uniform(-10, 10, (self.population_size, n_genes))\n",
    "\n",
    "    def _mutation(self, individual):\n",
    "        for i in range(len(individual)):\n",
    "            if np.random.rand() < self.mutation_rate:\n",
    "                individual += np.random.normal(0, 1, self.problem_dim)\n",
    "        return individual\n",
    "\n",
    "    def _crossover(self, parent1, parent2):\n",
    "        if np.random.rand() < self.crossover_rate:\n",
    "            crossover_point = np.random.randint(1, self.problem_dim - 1)\n",
    "            child1 = np.concatenate(\n",
    "                (parent1[:crossover_point], parent2[crossover_point:])\n",
    "            )\n",
    "            child2 = np.concatenate(\n",
    "                (parent2[:crossover_point], parent1[crossover_point:])\n",
    "            )\n",
    "            return child1, child2\n",
    "        return parent1, parent2\n",
    "\n",
    "    def _evaluate_population(self):\n",
    "        fitness = np.zeros(len(self.population))\n",
    "        for i, individual in enumerate(self.population):\n",
    "            fitness[i] = self.function_to_optimize(*individual)\n",
    "        self.hall_of_fame.append(np.argsort(fitness)[: int(self.population_size * 0.1)])\n",
    "        return fitness\n",
    "\n",
    "    def _tournament_selection(self, fitness):\n",
    "        fitness = 1 / fitness\n",
    "        probabilities = fitness / np.sum(fitness)\n",
    "        return self.population[\n",
    "            np.random.choice(\n",
    "                range(len(self.population)),\n",
    "                p=probabilities,\n",
    "                size=self.population_size - len(self.hall_of_fame[-1]),\n",
    "                replace=True,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def _select_individual(self, n=1):\n",
    "        return self.population[\n",
    "            np.random.choice(range(self.population_size), size=n, replace=False)\n",
    "        ]\n",
    "\n",
    "    def _visualize_individual(self, individual):\n",
    "        print(individual)\n",
    "\n",
    "    def optimize(self, function_to_optimize, silent=False):\n",
    "        # initialize the population\n",
    "        self.function_to_optimize = function_to_optimize\n",
    "        self.problem_dim = function_to_optimize.__code__.co_argcount\n",
    "        self._generate_population(self.problem_dim)\n",
    "        best_evaluation_in_iteration = []\n",
    "        best_solution_in_iteration = []\n",
    "\n",
    "        # main algorithm loop\n",
    "        for generation in range(self.number_of_generations):\n",
    "\n",
    "            # crossover\n",
    "            children = np.zeros(\n",
    "                (2 * len(range(0, self.population_size, 2)), self.problem_dim)\n",
    "            )\n",
    "            for i in range(0, self.population_size, 2):\n",
    "                parent1, parent2 = self._select_individual(n=2)\n",
    "                children[i], children[i + 1] = self._crossover(parent1, parent2)\n",
    "            self.population = np.vstack([self.population, children])\n",
    "\n",
    "            # mutation\n",
    "            # we mutate every individual in the population not random one\n",
    "            mutated = np.zeros((self.population_size, self.problem_dim))\n",
    "            for i in range(self.population_size):\n",
    "                individual = self._select_individual()\n",
    "                mutated[i] = self._mutation(individual)\n",
    "            self.population = np.vstack([self.population, mutated])\n",
    "\n",
    "            # evaluate the population and log the best solution\n",
    "            fitness = self._evaluate_population()\n",
    "            if not silent:\n",
    "                print(\n",
    "                    f\"iter: {generation}, best: {np.min(fitness)} for {['%.3f' % n for n in self.population[self.hall_of_fame[-1][0]]]}\"\n",
    "                )\n",
    "\n",
    "            best_evaluation_in_iteration.append(np.min(fitness))\n",
    "            best_solution_in_iteration.append(self.population[self.hall_of_fame[-1][0]])\n",
    "\n",
    "            # create new population\n",
    "            self.population = np.vstack(\n",
    "                [\n",
    "                    self.population[self.hall_of_fame[-1]],\n",
    "                    self._tournament_selection(fitness),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return best_evaluation_in_iteration, best_solution_in_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "auto_mpg = fetch_ucirepo(id=9)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = auto_mpg.data.features\n",
    "y = auto_mpg.data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "iris = fetch_ucirepo(id=53)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = iris.data.features\n",
    "y = iris.data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_large_train = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/nizwant/miowid/main/data/regression/multimodal-large-training.csv\"\n",
    ")\n",
    "multimodal_large_test = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/nizwant/miowid/main/data/regression/multimodal-large-test.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = multimodal_large_train.mean()\n",
    "std = multimodal_large_train.std()\n",
    "multimodal_large_train = (multimodal_large_train - mean) / std\n",
    "multimodal_large_test = (multimodal_large_test - mean) / std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algorytmyistdanych",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

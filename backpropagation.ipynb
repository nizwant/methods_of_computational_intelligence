{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    __slots__ = [\n",
    "        \"weights\",\n",
    "        \"biases\",\n",
    "        \"activation\",\n",
    "        \"biases_gradient\",\n",
    "        \"weights_gradient\",\n",
    "        \"activation_derivative\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, nodes_in, nodes_out, activation=\"sigmoid\"):\n",
    "        self.weights = np.random.normal(size=(nodes_in, nodes_out), scale=1)\n",
    "        self.biases = np.random.normal(size=(1, nodes_out))\n",
    "        self.biases_gradient = np.zeros(self.biases.shape)\n",
    "        self.weights_gradient = np.zeros(self.weights.shape)\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = self.sigmoid\n",
    "            self.activation_derivative = self.sigmoid_derivative\n",
    "        elif activation == \"linear\":\n",
    "            self.activation = self.linear\n",
    "            self.activation_derivative = self.linear_derivative\n",
    "\n",
    "    def calculate_layer(self, input):\n",
    "        \"\"\"\n",
    "        Calculate the output of the layer\n",
    "        Takes in a numpy array and returns a numpy array\n",
    "        \"\"\"\n",
    "        return self.activation(np.dot(input, self.weights) + self.biases)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function\n",
    "        Takes in a numpy array and returns a numpy array\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid derivative function\n",
    "        Takes in a numpy array and returns a numpy array\n",
    "        \"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def linear(self, x):\n",
    "        \"\"\"\n",
    "        Linear activation function\n",
    "        Takes in a numpy array and returns a numpy array\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "    def linear_derivative(self, x):\n",
    "        \"\"\"\n",
    "        Linear derivative function\n",
    "        Takes in a numpy array and returns a numpy array\n",
    "        \"\"\"\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    def report_layer(self, layer_num):\n",
    "        return (\n",
    "            f\"Layer number {layer_num}\\nWeights\\n{self.weights}\\nbiases\\n{self.biases}\"\n",
    "        )\n",
    "\n",
    "    def apply_gradient(self, learning_rate):\n",
    "        self.weights -= learning_rate * self.weights_gradient\n",
    "        self.biases -= learning_rate * self.biases_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    __slots__ = [\"hidden_layers\", \"layers\"]\n",
    "\n",
    "    def __init__(self, hidden_layers, input_size, output_size):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.layers = []\n",
    "\n",
    "        # Create the input layer\n",
    "        input_layer = Layer(1, hidden_layers[0])\n",
    "        self.layers.append(input_layer)\n",
    "\n",
    "        # Create the hidden layers\n",
    "        for input_size, output_size in zip(hidden_layers, hidden_layers[1:]):\n",
    "            self.layers.append(Layer(input_size, output_size))\n",
    "\n",
    "        # Create the output layer\n",
    "        output_layer = Layer(hidden_layers[-1], 1, activation=\"linear\")\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes a input and returns the output of the network\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.calculate_layer(input)\n",
    "        return input\n",
    "\n",
    "    def train(\n",
    "        self, input, output, learning_rate=0.003, batch_size_frac=0.1, epochs=30000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train the network on a given dataset\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            merged = input.to_frame().join(output)\n",
    "            merged = merged.sample(frac=batch_size_frac)\n",
    "            self.calculate_gradient(merged[\"x\"], merged[\"y\"])\n",
    "            self.apply_gradient(learning_rate)\n",
    "            mse = self.mean_squared_error(merged[\"x\"], merged[\"y\"])\n",
    "            if epoch % 2000 == 0:\n",
    "                print(f\"Epoch {epoch} MSE: {mse}\")\n",
    "\n",
    "    def backpropagation(self, input, output):\n",
    "        \"\"\"\n",
    "        Perform backpropagation on the network\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def mean_squared_error(self, input, output):\n",
    "        \"\"\"\n",
    "        Calculate the mean squared error of the network on a given dataset and output\n",
    "        \"\"\"\n",
    "        mse = []\n",
    "        for i, j in zip(input, output):\n",
    "            mse.append((j - self.forward(i)) ** 2)\n",
    "        return np.mean(mse)\n",
    "\n",
    "    def mean_squared_error_gradient(self, predicted, true):\n",
    "        \"\"\"\n",
    "        Calculate the gradient of the mean squared error\n",
    "        \"\"\"\n",
    "        return 2 * (predicted - true)\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Visualize the network architecture\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(layer.report_layer(i))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    def apply_gradient(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.apply_gradient(learning_rate)\n",
    "\n",
    "    def calculate_gradient(self, input, output):\n",
    "        \"\"\"\n",
    "        Calculate the gradient of the network\n",
    "        \"\"\"\n",
    "        h = 0.0001\n",
    "        original_mse = self.mean_squared_error(input, output)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            for i in range(layer.weights.shape[0]):\n",
    "                for j in range(layer.weights.shape[1]):\n",
    "                    layer.weights[i, j] += h\n",
    "                    new_mse = self.mean_squared_error(input, output)\n",
    "                    layer.weights_gradient[i, j] = (new_mse - original_mse) / h\n",
    "                    layer.weights[i, j] -= h\n",
    "\n",
    "            for i in range(layer.biases.shape[0]):\n",
    "                for j in range(layer.biases.shape[1]):\n",
    "                    layer.biases[i, j] += h\n",
    "                    new_mse = self.mean_squared_error(input, output)\n",
    "                    layer.biases_gradient[i, j] = (new_mse - original_mse) / h\n",
    "                    layer.biases[i, j] -= h"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
